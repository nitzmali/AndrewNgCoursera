{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import plotly as pl\n",
    "import scipy.optimize as opt\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('machine-learning-ex8\\ex8\\ex8_movies.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=data['Y']\n",
    "R=data['R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.52067868504772"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y[0,R[0,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Users')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Y)\n",
    "plt.ylabel('Movies')\n",
    "plt.xlabel('Users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collaborative cost function\n",
    "\n",
    "data1=loadmat('machine-learning-ex8\\ex8\\ex8_movieParams.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3) (4, 3) (5, 4) (5, 4)\n"
     ]
    }
   ],
   "source": [
    "X,Theta,num_users,num_movies,num_features=data1['X'],data1['Theta'],data1['num_users'],data1['num_movies'],data1['num_features']\n",
    "#print(X.shape,Theta.shape,num_users.shape,num_movies.shape,num_features.shape)\n",
    "\n",
    "#reduce the data set size so that this runs faster\n",
    "num_users = 4\n",
    "num_movies = 5\n",
    "num_features = 3\n",
    "\n",
    "X = X[0:num_movies, 0:num_features]\n",
    "Theta = Theta[0:num_users, 0:num_features]\n",
    "Y = Y[0:num_movies, 0:num_users]\n",
    "R = R[0:num_movies, 0:num_users]\n",
    "\n",
    "print(X.shape,Theta.shape,Y.shape,R.shape)\n",
    "params=np.concatenate([X.ravel(),Theta.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def costfunction(params,Y,R,num_users,num_movies,num_features,lmbda=0.0):\n",
    "    X=params[:num_movies*num_features].reshape(num_movies,num_features)\n",
    "    Theta=params[num_movies*num_features:].reshape(num_users,num_features)\n",
    "    print(X.shape,Theta.shape,Y.shape,R.shape)\n",
    "    J=0\n",
    "    X_grad=np.zeros(X.shape)\n",
    "    Theta_grad=np.zeros(Theta.shape)\n",
    "    \n",
    "    J=1/2*(np.sum(np.square(np.dot(X,Theta.T)-Y)*R))+(lmbda/2)*np.sum(np.square(X))+(lmbda/2)*np.sum(np.square(Theta))\n",
    "    print(J)\n",
    "    \n",
    "    for i in range(R.shape[0]):\n",
    "        idx =np.where(R[i,:]==1)[0]\n",
    "        theta_temp=Theta[idx,:]\n",
    "        y_temp =Y[i,idx]\n",
    "        \n",
    "        #print(theta_temp.shape,X[i,:].shape,y_temp.shape)\n",
    "        X_grad[i,:]=np.dot((np.dot(X[i,:],theta_temp.T)-y_temp),theta_temp)+lmbda*X[i,:]\n",
    "    \n",
    "    for j in range(R.shape[1]):\n",
    "        idx =np.where(R[:,j]==1)[0]\n",
    "        x_temp=X[idx,:]\n",
    "        y_temp=Y[idx,j]\n",
    "        #print(Theta[j].shape,x_temp.shape,y_temp.shape)\n",
    "        Theta_grad[j,:]=np.dot((np.dot(x_temp,Theta[j,:])-y_temp),x_temp)+lmbda*Theta[j,:]\n",
    "        \n",
    "    grad=np.concatenate([X_grad.ravel(),Theta_grad.ravel()]) \n",
    "    return J, grad \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNumericalGradient(J, theta, e=1e-4):\n",
    "    \"\"\"\n",
    "    Computes the gradient using \"finite differences\" and gives us a numerical estimate of the\n",
    "    gradient.\n",
    "    Parameters\n",
    "    ----------\n",
    "    J : func\n",
    "        The cost function which will be used to estimate its numerical gradient.\n",
    "    theta : array_like\n",
    "        The one dimensional unrolled network parameters. The numerical gradient is computed at\n",
    "         those given parameters.\n",
    "    e : float (optional)\n",
    "        The value to use for epsilon for computing the finite difference.\n",
    "    Returns\n",
    "    -------\n",
    "    numgrad : array_like\n",
    "        The numerical gradient with respect to theta. Has same shape as theta.\n",
    "    Notes\n",
    "    -----\n",
    "    The following code implements numerical gradient checking, and\n",
    "    returns the numerical gradient. It sets `numgrad[i]` to (a numerical\n",
    "    approximation of) the partial derivative of J with respect to the\n",
    "    i-th input argument, evaluated at theta. (i.e., `numgrad[i]` should\n",
    "    be the (approximately) the partial derivative of J with respect\n",
    "    to theta[i].)\n",
    "    \"\"\"\n",
    "    numgrad = np.zeros(theta.shape)\n",
    "    perturb = np.diag(e * np.ones(theta.shape))\n",
    "    for i in range(theta.size):\n",
    "        loss1, _ = J(theta - perturb[:, i])\n",
    "        loss2, _ = J(theta + perturb[:, i])\n",
    "        numgrad[i] = (loss2 - loss1)/(2*e)\n",
    "    return numgrad\n",
    "\n",
    "\n",
    "def checkCostFunction(cofiCostFunc, lambda_=0.):\n",
    "    \"\"\"\n",
    "    Creates a collaborative filtering problem to check your cost function and gradients.\n",
    "    It will output the  analytical gradients produced by your code and the numerical gradients\n",
    "    (computed using computeNumericalGradient). These two gradient computations should result\n",
    "    in very similar values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cofiCostFunc: func\n",
    "        Implementation of the cost function.\n",
    "    lambda_ : float, optional\n",
    "        The regularization parameter.\n",
    "    \"\"\"\n",
    "    # Create small problem\n",
    "    X_t = np.random.rand(4, 3)\n",
    "    Theta_t = np.random.rand(5, 3)\n",
    "\n",
    "    # Zap out most entries\n",
    "    Y = np.dot(X_t, Theta_t.T)\n",
    "    Y[np.random.rand(*Y.shape) > 0.5] = 0\n",
    "    R = np.zeros(Y.shape)\n",
    "    R[Y != 0] = 1\n",
    "\n",
    "    # Run Gradient Checking\n",
    "    X = np.random.randn(*X_t.shape)\n",
    "    Theta = np.random.randn(*Theta_t.shape)\n",
    "    num_movies, num_users = Y.shape\n",
    "    num_features = Theta_t.shape[1]\n",
    "\n",
    "    params = np.concatenate([X.ravel(), Theta.ravel()])\n",
    "    numgrad = computeNumericalGradient(\n",
    "        lambda x: cofiCostFunc(x, Y, R, num_users, num_movies, num_features, lambda_), params)\n",
    "\n",
    "    cost, grad = cofiCostFunc(params, Y, R, num_users,num_movies, num_features, lambda_)\n",
    "\n",
    "    print(np.stack([numgrad, grad], axis=1))\n",
    "    print('\\nThe above two columns you get should be very similar.'\n",
    "          '(Left-Your Numerical Gradient, Right-Analytical Gradient)')\n",
    "\n",
    "    diff = np.linalg.norm(numgrad-grad)/np.linalg.norm(numgrad+grad)\n",
    "    print('If your cost function implementation is correct, then '\n",
    "          'the relative difference will be small (less than 1e-9).')\n",
    "    print('\\nRelative Difference: %g' % diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3) (4, 3) (5, 4) (5, 4)\n",
      "22.224603725685675\n"
     ]
    }
   ],
   "source": [
    "J,grad=costfunction(params,Y,R,num_users,num_movies,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkCostFunction(costfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3) (4, 3) (5, 4) (5, 4)\n",
      "31.344056244274217\n"
     ]
    }
   ],
   "source": [
    "#regularized gradient \n",
    "J,grad=costfunction(params,Y,R,num_users,num_movies,num_features,1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01904437710818\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01902859111211\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019349298005174\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01872367063511\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.0188160791858\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01925689302611\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.018708188142064\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019364788161404\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.018139768491345\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01993323659614\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.0187227649418\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019350201122485\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.018358610232745\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01971439555965\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01794050168712\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.02013249803223\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01878120557111\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01929175779456\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.0190676805809\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01900533798375\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01911748398323\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01895548681796\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01900571685149\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01906725541816\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01855615245121\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01951681307848\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01753514011854\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.020537875198656\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.018395086084766\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019677891397954\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01895104549841\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019121919063465\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019309954651106\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.0187630481112\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01899974892773\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01907322838829\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01886451698465\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01920844422651\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01906981336428\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019003149876994\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01902477897287\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.0190481821987\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01868760275808\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019385360776326\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01874516539714\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01932783360919\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01897596191601\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019097000934124\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01898743555948\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.019085526922325\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01875190880676\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01932107552977\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01922746278184\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.0188454998564\n",
      "(4, 3) (5, 3) (4, 5) (4, 5)\n",
      "33.01903647306318\n",
      "[[-0.07892998 -0.07892998]\n",
      " [-3.12813685 -3.12813685]\n",
      " [ 2.2040692   2.2040692 ]\n",
      " [ 3.2830001   3.2830001 ]\n",
      " [ 8.96734052  8.96734052]\n",
      " [ 3.1371809   3.1371809 ]\n",
      " [ 6.77892663  6.77892663]\n",
      " [10.95998173 10.95998173]\n",
      " [ 2.55276112  2.55276112]\n",
      " [-0.31171299 -0.31171299]\n",
      " [-0.80998583 -0.80998583]\n",
      " [ 0.30769283  0.30769283]\n",
      " [ 4.80330314  4.80330314]\n",
      " [15.0136754  15.0136754 ]\n",
      " [ 6.41402657  6.41402657]\n",
      " [ 0.85436783  0.85436783]\n",
      " [-2.7345327  -2.7345327 ]\n",
      " [ 0.3673973   0.3673973 ]\n",
      " [ 1.71963621  1.71963621]\n",
      " [-0.33331744 -0.33331744]\n",
      " [ 0.11701613  0.11701613]\n",
      " [ 3.48879009  3.48879009]\n",
      " [ 2.91334106  2.91334106]\n",
      " [ 0.60519509  0.60519509]\n",
      " [ 0.49045681  0.49045681]\n",
      " [ 2.84583362  2.84583362]\n",
      " [-1.90981463 -1.90981463]]\n",
      "\n",
      "The above two columns you get should be very similar.(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "If your cost function implementation is correct, then the relative difference will be small (less than 1e-9).\n",
      "\n",
      "Relative Difference: 1.85213e-12\n"
     ]
    }
   ],
   "source": [
    "checkCostFunction(costfunction,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning about movie recommendations\n",
    "from os.path import join\n",
    "from matplotlib import pyplot\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "with open(('machine-learning-ex8\\ex8\\movie_ids.txt'),  encoding='ISO-8859-1') as fid:\n",
    "    movies = fid.readlines()\n",
    "\n",
    "movienames =[]\n",
    "\n",
    "for each in movies:\n",
    "    parts=each.split()\n",
    "    movienames.append(' '.join(parts[1:]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeratings(Y,R):\n",
    "    m,n=Y.shape\n",
    "    Ymean =np.zeros(m)\n",
    "    Ynorm =np.zeros(Y.shape)\n",
    "    #print(Ymean.shape,Ynorm.shape)\n",
    "    \n",
    "    for i in range(m):\n",
    "        idx = np.where(R[i,:]==1)\n",
    "        Ymean[i]=np.mean(Y[i,idx])\n",
    "        Ynorm[i,idx]=Y[i,idx]-Ymean[i]\n",
    "    return Ymean,Ynorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings:\n",
      "-----------------\n",
      "Rated 4 stars: Toy Story (1995)\n",
      "Rated 3 stars: Twelve Monkeys (1995)\n",
      "Rated 5 stars: Usual Suspects, The (1995)\n",
      "Rated 4 stars: Outbreak (1995)\n",
      "Rated 5 stars: Shawshank Redemption, The (1994)\n",
      "Rated 3 stars: While You Were Sleeping (1995)\n",
      "Rated 5 stars: Forrest Gump (1994)\n",
      "Rated 2 stars: Silence of the Lambs, The (1991)\n",
      "Rated 4 stars: Alien (1979)\n",
      "Rated 5 stars: Die Hard 2 (1990)\n",
      "Rated 5 stars: Sphere (1998)\n"
     ]
    }
   ],
   "source": [
    "n_m=len(movienames)\n",
    "my_ratings =np.zeros(len(movienames))\n",
    "\n",
    "my_ratings[0] = 4\n",
    "\n",
    "# Or suppose did not enjoy Silence of the Lambs (1991), you can set\n",
    "my_ratings[97] = 2\n",
    "\n",
    "# We have selected a few movies we liked / did not like and the ratings we\n",
    "# gave are as follows:\n",
    "my_ratings[6] = 3\n",
    "my_ratings[11]= 5\n",
    "my_ratings[53] = 4\n",
    "my_ratings[63] = 5\n",
    "my_ratings[65] = 3\n",
    "my_ratings[68] = 5\n",
    "my_ratings[182] = 4\n",
    "my_ratings[225] = 5\n",
    "my_ratings[354] = 5\n",
    "\n",
    "print('New user ratings:')\n",
    "print('-----------------')\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0:\n",
    "        print('Rated %d stars: %s' % (my_ratings[i], movienames[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add our ratings to the dataset\n",
    "\n",
    "data = loadmat('machine-learning-ex8\\ex8\\ex8_movies.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682, 944)\n"
     ]
    }
   ],
   "source": [
    "Y=data['Y']\n",
    "R=data['R']\n",
    "\n",
    "Y = np.hstack([my_ratings[:, None], Y])\n",
    "R = np.hstack([(my_ratings > 0)[:, None], R])\n",
    "\n",
    "#  Normalize Ratings\n",
    "Ymean, Ynorm = normalizeratings(Y, R)\n",
    "\n",
    "#  Useful Values\n",
    "num_movies, num_users = Y.shape\n",
    "num_features = 10\n",
    "\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.randn(num_movies,num_features)\n",
    "Theta=np.random.randn(num_users,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Initial Parameters (Theta, X)\n",
    "X = np.random.randn(num_movies, num_features)\n",
    "Theta = np.random.randn(num_users, num_features)\n",
    "\n",
    "initial_parameters = np.concatenate([X.ravel(), Theta.ravel()])\n",
    "\n",
    "# Set options for scipy.optimize.minimize\n",
    "options = {'maxiter': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "676741.8148303152\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "676741.8140027111\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "676741.8147017083\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "320723.510145887\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "320723.50833766407\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "320723.5085614511\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "320723.509887615\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "155025.7378132782\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "1018978.8979593107\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "151492.23339396936\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.8022897424\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.80080679331\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.80186922396\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.80138400255\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.80201090308\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.80217263343\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.80218743434\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.80218622355\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.80220625146\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "112604.80246088804\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "58238.223864136104\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "58238.22161582536\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "58238.22221363205\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "58238.223325612176\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "58238.222902116904\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "58238.221511077645\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "49456.03658026808\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "50912.108828722674\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "49199.557774125096\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "49199.55561382742\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "49199.55648972801\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "49199.5572437522\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "49199.55703747086\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "49199.55696001942\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "49199.55594110843\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "156514.86770393385\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "49777.39938291263\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44640.660261008656\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "45236.253719476845\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.61223967159\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.611014481925\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.61141505017\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.61194907967\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.61190790092\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.61193924894\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.611722860456\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.612007839816\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.61204543223\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.61198838123\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.6118875521\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.61170364548\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.61016586511\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "44558.50723278395\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "394901175.42749923\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "24817089.88574826\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "1593191.194019402\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "135480.44884069063\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "46499.19976765168\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "42694.643449104115\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "43233.97734578206\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "42727.330965624045\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "42662.79048937744\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "42662.78908939862\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "42662.7897456089\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "42662.79005537752\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.89126480242\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.89094322558\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.890898872785\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.89102537778\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.89115654896\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.8911845216\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.89121172033\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.89121987402\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.891223626124\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.89119569654\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "41376.89115695174\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39960.67970651525\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "40108.76191331703\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28655812051\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28639756529\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28648860975\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28650238514\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28652344898\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28653660575\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28654428671\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28652130935\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28653371019\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.28653489625\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39867.286553038335\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.87570302506\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.87548548064\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.87567911715\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.875683311635\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.87568893398\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.875694629365\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.875690673056\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.87568947611\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.875691154295\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39448.87569602836\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39225.90344222603\n",
      "(1682, 10) (944, 10) (1682, 944) (1682, 944)\n",
      "39225.90344222603\n"
     ]
    }
   ],
   "source": [
    "# Set Regularization\n",
    "import scipy.optimize as opt\n",
    "lambda_ = 10\n",
    "res = opt.minimize(lambda x: costfunction(x, Ynorm, R, num_users,\n",
    "                                               num_movies, num_features, lambda_),\n",
    "                        initial_parameters,\n",
    "                        method='TNC',\n",
    "                        jac=True,\n",
    "                        options=options)\n",
    "theta = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender system learning completed.\n"
     ]
    }
   ],
   "source": [
    " #Unfold the returned theta back into U and W\n",
    "X = theta[:num_movies*num_features].reshape(num_movies, num_features)\n",
    "Theta = theta[num_movies*num_features:].reshape(num_users, num_features)\n",
    "\n",
    "print('Recommender system learning completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.dot(X, Theta.T)\n",
    "my_predictions = p[:, 0] + Ymean\n",
    "\n",
    "ix=np.argsort(my_predictions)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations for you:\n",
      "----------------------------\n",
      "Predicting rating 5.0 for movie Someone Else's America (1995)\n",
      "Predicting rating 5.0 for movie Santa with Muscles (1996)\n",
      "Predicting rating 5.0 for movie Star Kid (1997)\n",
      "Predicting rating 5.0 for movie Great Day in Harlem, A (1994)\n",
      "Predicting rating 5.0 for movie Marlene Dietrich: Shadow and Light (1996)\n",
      "Predicting rating 5.0 for movie Prefontaine (1997)\n",
      "Predicting rating 5.0 for movie Saint of Fort Washington, The (1993)\n",
      "Predicting rating 5.0 for movie Aiqing wansui (1994)\n",
      "Predicting rating 5.0 for movie They Made Me a Criminal (1939)\n",
      "Predicting rating 5.0 for movie Entertaining Angels: The Dorothy Day Story (1996)\n",
      "\n",
      "Original ratings provided:\n",
      "--------------------------\n",
      "Rated 4 for Toy Story (1995)\n",
      "Rated 3 for Twelve Monkeys (1995)\n",
      "Rated 5 for Usual Suspects, The (1995)\n",
      "Rated 4 for Outbreak (1995)\n",
      "Rated 5 for Shawshank Redemption, The (1994)\n",
      "Rated 3 for While You Were Sleeping (1995)\n",
      "Rated 5 for Forrest Gump (1994)\n",
      "Rated 2 for Silence of the Lambs, The (1991)\n",
      "Rated 4 for Alien (1979)\n",
      "Rated 5 for Die Hard 2 (1990)\n",
      "Rated 5 for Sphere (1998)\n"
     ]
    }
   ],
   "source": [
    "print('Top recommendations for you:')\n",
    "print('----------------------------')\n",
    "for i in range(10):\n",
    "    j = ix[i]\n",
    "    print('Predicting rating %.1f for movie %s' % (my_predictions[j], movienames[j]))\n",
    "\n",
    "print('\\nOriginal ratings provided:')\n",
    "print('--------------------------')\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0:\n",
    "        print('Rated %d for %s' % (my_ratings[i], movienames[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
